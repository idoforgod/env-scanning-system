#!/usr/bin/env python3
"""
Batch Content Fetcher
=====================
ëŒ€ëŸ‰ URLì˜ ë³¸ë¬¸ ìˆ˜ì§‘ì„ ì•ˆì „í•œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬

êµ¬ì¡°ì  ë¬¸ì œ 5.1 í•´ê²°:
- "ì—ì´ì „íŠ¸ ì»¨í…ìŠ¤íŠ¸ í•œê³„" ë¬¸ì œ ë°©ì§€
- ë°°ì¹˜ í¬ê¸°ë¥¼ 10ê°œ ì´í•˜ë¡œ ì œí•œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê³¼ë¶€í•˜ ë°©ì§€

Usage:
    python batch_content_fetcher.py --date 2026-01-14 --batch-size 8
    python batch_content_fetcher.py --input urls.json --output articles.json
"""

import argparse
import json
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ë°°ì¹˜ ì„¤ì • (êµ¬ì¡°ì  ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì œí•œ)
MAX_BATCH_SIZE = 10  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸° (ì»¨í…ìŠ¤íŠ¸ í•œê³„ ë°©ì§€)
DEFAULT_BATCH_SIZE = 8  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° (ì•ˆì „ ë§ˆì§„)
REQUEST_TIMEOUT = 15  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
BATCH_DELAY = 2  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
MIN_CONTENT_LENGTH = 200  # ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´ (ì)


@dataclass
class FetchResult:
    """ë³¸ë¬¸ ìˆ˜ì§‘ ê²°ê³¼"""

    url: str
    success: bool
    title: str | None
    content: str | None
    content_length: int
    error: str | None
    fetch_time_ms: int


class BatchContentFetcher:
    """ë°°ì¹˜ ë‹¨ìœ„ ë³¸ë¬¸ ìˆ˜ì§‘ê¸°"""

    def __init__(self, batch_size: int = DEFAULT_BATCH_SIZE):
        """
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸° (ìµœëŒ€ 10)
        """
        if batch_size > MAX_BATCH_SIZE:
            print(f"âš ï¸ ë°°ì¹˜ í¬ê¸° {batch_size}ëŠ” ìµœëŒ€ê°’ {MAX_BATCH_SIZE}ë¡œ ì œí•œë©ë‹ˆë‹¤.")
            batch_size = MAX_BATCH_SIZE

        self.batch_size = batch_size
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            }
        )

    def _extract_content(self, html: str, url: str) -> tuple[str | None, str | None]:
        """
        HTMLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ

        Returns:
            (title, content)
        """
        try:
            soup = BeautifulSoup(html, "html.parser")

            # ì œëª© ì¶”ì¶œ
            title = None
            if soup.title:
                title = soup.title.string
            if not title:
                h1 = soup.find("h1")
                if h1:
                    title = h1.get_text(strip=True)

            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for tag in soup(["script", "style", "nav", "header", "footer", "aside", "ads"]):
                tag.decompose()

            # ë³¸ë¬¸ ì¶”ì¶œ (article > main > body ìš°ì„ ìˆœìœ„)
            content = None
            for selector in ["article", "main", "[role='main']", ".content", "#content", "body"]:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(separator="\n", strip=True)
                    if len(content) > MIN_CONTENT_LENGTH:
                        break

            # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì „ì²´ body ì‚¬ìš©
            if not content or len(content) < MIN_CONTENT_LENGTH:
                content = soup.get_text(separator="\n", strip=True)

            return title, content

        except Exception as e:
            print(f"  Content extraction error: {e}")
            return None, None

    def fetch_url(self, url: str) -> FetchResult:
        """
        ë‹¨ì¼ URL ë³¸ë¬¸ ìˆ˜ì§‘

        Args:
            url: ìˆ˜ì§‘í•  URL

        Returns:
            FetchResult
        """
        start_time = time.time()

        try:
            response = self.session.get(url, timeout=REQUEST_TIMEOUT)
            fetch_time = int((time.time() - start_time) * 1000)

            if response.status_code != 200:
                return FetchResult(
                    url=url,
                    success=False,
                    title=None,
                    content=None,
                    content_length=0,
                    error=f"HTTP_{response.status_code}",
                    fetch_time_ms=fetch_time,
                )

            # ì¸ì½”ë”© ì²˜ë¦¬
            response.encoding = response.apparent_encoding or "utf-8"

            # ë³¸ë¬¸ ì¶”ì¶œ
            title, content = self._extract_content(response.text, url)

            if not content or len(content) < MIN_CONTENT_LENGTH:
                return FetchResult(
                    url=url,
                    success=False,
                    title=title,
                    content=None,
                    content_length=len(content) if content else 0,
                    error="CONTENT_TOO_SHORT",
                    fetch_time_ms=fetch_time,
                )

            return FetchResult(
                url=url,
                success=True,
                title=title,
                content=content[:10000],  # ìµœëŒ€ 10000ìë¡œ ì œí•œ
                content_length=len(content),
                error=None,
                fetch_time_ms=fetch_time,
            )

        except requests.exceptions.Timeout:
            return FetchResult(
                url=url,
                success=False,
                title=None,
                content=None,
                content_length=0,
                error="TIMEOUT",
                fetch_time_ms=REQUEST_TIMEOUT * 1000,
            )
        except requests.exceptions.ConnectionError:
            return FetchResult(
                url=url,
                success=False,
                title=None,
                content=None,
                content_length=0,
                error="CONNECTION_ERROR",
                fetch_time_ms=int((time.time() - start_time) * 1000),
            )
        except Exception as e:
            return FetchResult(
                url=url,
                success=False,
                title=None,
                content=None,
                content_length=0,
                error=f"ERROR: {str(e)[:50]}",
                fetch_time_ms=int((time.time() - start_time) * 1000),
            )

    def fetch_batch(self, urls: list[str], batch_num: int = 1) -> list[FetchResult]:
        """
        ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬

        Args:
            urls: URL ëª©ë¡ (ìµœëŒ€ batch_sizeê°œ)
            batch_num: ë°°ì¹˜ ë²ˆí˜¸

        Returns:
            FetchResult ëª©ë¡
        """
        results = []
        print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ({len(urls)}ê°œ URL)...")

        for i, url in enumerate(urls, 1):
            print(f"  [{i}/{len(urls)}] {url[:60]}...", end=" ")
            result = self.fetch_url(url)

            if result.success:
                print(f"âœ“ ({result.content_length}ì)")
            else:
                print(f"âœ— {result.error}")

            results.append(result)
            time.sleep(0.5)  # Rate limiting

        return results

    def fetch_all(self, urls: list[str]) -> dict:
        """
        ì „ì²´ URL ë°°ì¹˜ ë¶„í•  ì²˜ë¦¬

        Args:
            urls: ì „ì²´ URL ëª©ë¡

        Returns:
            {
                "articles": [...],
                "stats": {...}
            }
        """
        total_urls = len(urls)
        num_batches = (total_urls + self.batch_size - 1) // self.batch_size

        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print("  Batch Content Fetcher")
        print(f"  ì´ URL: {total_urls}ê°œ")
        print(f"  ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œ (ìµœëŒ€ {MAX_BATCH_SIZE})")
        print(f"  ì´ ë°°ì¹˜ ìˆ˜: {num_batches}ê°œ")
        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        all_results = []
        start_time = time.time()

        for batch_num in range(num_batches):
            batch_start = batch_num * self.batch_size
            batch_end = min(batch_start + self.batch_size, total_urls)
            batch_urls = urls[batch_start:batch_end]

            results = self.fetch_batch(batch_urls, batch_num + 1)
            all_results.extend(results)

            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
            if batch_num < num_batches - 1:
                print(f"  â³ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {BATCH_DELAY}ì´ˆ ëŒ€ê¸°...")
                time.sleep(BATCH_DELAY)

        elapsed = time.time() - start_time

        # ê²°ê³¼ ì •ë¦¬
        successful = [r for r in all_results if r.success]
        failed = [r for r in all_results if not r.success]

        # ê¸°ì‚¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        articles = []
        for result in successful:
            articles.append(
                {
                    "url": result.url,
                    "original_title": result.title,
                    "original_content": result.content,
                    "content_length": result.content_length,
                    "fetched_at": datetime.now().isoformat(),
                }
            )

        # ì—ëŸ¬ í†µê³„
        error_counts: dict[str, int] = {}
        for result in failed:
            error = result.error or "UNKNOWN"
            error_counts[error] = error_counts.get(error, 0) + 1

        output = {
            "fetched_at": datetime.now().isoformat(),
            "batch_size": self.batch_size,
            "total_batches": num_batches,
            "total_time_sec": round(elapsed, 2),
            "stats": {
                "total_urls": total_urls,
                "success_count": len(successful),
                "fail_count": len(failed),
                "success_rate": round(len(successful) / total_urls * 100, 1) if total_urls else 0,
                "errors_by_type": error_counts,
            },
            "articles": articles,
            "failed_urls": [{"url": r.url, "error": r.error} for r in failed],
        }

        # ìš”ì•½ ì¶œë ¥
        print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print("  ì™„ë£Œ!")
        print(f"  ì„±ê³µ: {len(successful)}/{total_urls} ({output['stats']['success_rate']}%)")
        print(f"  ì‹¤íŒ¨: {len(failed)}")
        print(f"  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        return output


def main():
    parser = argparse.ArgumentParser(description="Batch Content Fetcher")
    parser.add_argument("--date", help="Scan date (YYYY-MM-DD)")
    parser.add_argument("--input", help="Input JSON file with URLs")
    parser.add_argument("--output", help="Output JSON file for articles")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE,
        help=f"Batch size (max {MAX_BATCH_SIZE})",
    )

    args = parser.parse_args()

    fetcher = BatchContentFetcher(batch_size=args.batch_size)

    # ì…ë ¥ íŒŒì¼ ê²°ì •
    if args.date:
        year, month, day = args.date.split("-")
        base_path = Path(__file__).parent.parent.parent.parent
        input_path = base_path / "data" / year / month / day / "raw" / f"validated-urls-{args.date}.json"
        if not input_path.exists():
            input_path = base_path / "data" / year / month / day / "raw" / f"urls-{args.date}.json"
        output_path = base_path / "data" / year / month / day / "raw" / f"articles-{args.date}.json"
    elif args.input:
        input_path = Path(args.input)
        output_path = Path(args.output) if args.output else input_path.with_name("articles.json")
    else:
        parser.print_help()
        sys.exit(1)

    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}")
        sys.exit(1)

    # URL ë¡œë“œ
    with open(input_path, encoding="utf-8") as f:
        data = json.load(f)

    # URL ì¶”ì¶œ (í†µì¼ëœ í˜•ì‹: urls ë°°ì—´)
    if isinstance(data, list):
        url_entries = data
    elif isinstance(data, dict):
        # í†µì¼ëœ íŒŒì´í”„ë¼ì¸ í˜•ì‹: "urls" ë°°ì—´ ì‚¬ìš©
        url_entries = data.get("urls", data.get("articles", []))
    else:
        print("Error: Invalid input format")
        sys.exit(1)

    # URL ë¬¸ìì—´ ì¶”ì¶œ
    urls = [entry.get("url", entry) if isinstance(entry, dict) else entry for entry in url_entries]

    # ë³¸ë¬¸ ìˆ˜ì§‘
    result = fetcher.fetch_all(urls)

    # ê²°ê³¼ ì €ì¥
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    main()
